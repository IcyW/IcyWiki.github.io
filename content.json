{"meta":{"title":"Icy's wiki","subtitle":"Life goes on, learning goes on","description":"生命不息，学习不止","author":"Icy","url":"icyhh.xyz","root":"/"},"pages":[{"title":"Categories","date":"2020-09-23T08:41:04.958Z","updated":"2020-09-23T08:41:04.958Z","comments":true,"path":"categories/index.html","permalink":"icyhh.xyz/categories/index.html","excerpt":"","text":""},{"title":"About","date":"2020-09-23T08:41:04.958Z","updated":"2020-09-23T08:41:04.958Z","comments":true,"path":"about/index.html","permalink":"icyhh.xyz/about/index.html","excerpt":"","text":""},{"title":"Tags","date":"2020-09-23T08:41:04.959Z","updated":"2020-09-23T08:41:04.959Z","comments":true,"path":"tags/index.html","permalink":"icyhh.xyz/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"关于持续交付中Git分支管理的思考","slug":"GitBranch-management","date":"2020-09-22T23:36:58.000Z","updated":"2020-09-23T08:41:04.955Z","comments":true,"path":"2020/09/23/GitBranch-management/","link":"","permalink":"icyhh.xyz/2020/09/23/GitBranch-management/","excerpt":"","text":"Aim at always writing production-ready code. 一、背景&nbsp; &nbsp; &nbsp; &nbsp; 提升研发效率（EP， Engineering Productivity），建立CI/CD体系，让持续自动化和持续监控贯穿于应用的整个生命周期已经成为有技术追求的攻城狮们的共识。&nbsp; &nbsp; &nbsp; &nbsp; 持续交付是对整个软件交付模式的变革，涉及到的内容非常多、非常广，在这个模型中大概有二十多个关键点。根据[EPC（Engineering Productivity Certification） 1.0]的要求，需要覆盖到的有以下12个维度： 0、需求协作管理 1、配置管理 2、制品库管理 3、分支管理 4、代码质量管理 5、测试管理 6、持续集成管理 7、自动化测试 8、开发测试与测试环境管理 9、发布 10、架构 11、数据觉察&nbsp; &nbsp; &nbsp; &nbsp; 虽然距离这些概念的提出已经有段时间了，对相关实践如何落地，大家大多处于探索、转变的阶段。经过这段时间的宅家分析与痛点沟通之后，我整理了一些想法，针对模型中的「分支管理」这一维度聚焦来谈谈。 二、现状分析&nbsp; &nbsp; &nbsp; &nbsp; 我们所对接的业务方产品形态十分多样，从移动端iOS、Android，到微信小程序、H5页面，再到PC web等等，每个开发团队都在历史的长河中形成了各自不同的开发习惯和分支管理策略。 &nbsp; &nbsp; &nbsp; &nbsp; 以其中一个典型的项目为例，我调用了开放API简单分析了一下项目（已脱敏）当前的分支状况。 分支生存周期统计壹&nbsp; &nbsp; &nbsp; &nbsp; 首先我拉取了项目中所有分支的信息，简单画出它们从被创建（begin_time）到销毁（delete_time）总共存在了多长时间；如果还未销毁，就计算到今天为止。 虽然是新建不久的项目，但是分支已经有182个，其中有五个存在超过了100天。还有31%的分支超过了一个月。 说不定只是因为没有约定好删除分支的规范，而非真的有3成的需求开发时间超过一个月呢？ 贰&nbsp; &nbsp; &nbsp; &nbsp; 所以假设不再有提交的分支其实是可以被废弃的。我又计算了一下分支从被创建（begin_time）到销毁（last_push_time）总共存在了多长时间。 虽然分支的使用时间的确是缩短了一些，可是只有40%的分支存在天数小于一周，剩下的长周期分支中依然有2成存在时间大于一个月。 叁&nbsp; &nbsp; &nbsp; &nbsp; 接下来再仔细看看这些「超长周期」的分支是什么情况。 在这个项目中，tiyan分支是作为类似发布分支的存在，master分支退居二线做开发使用，而test分支存放的是隔离开的单元测试和接口测试等等代码；后续存在了超过五十天的大部分是个人使用的bugfix分支。 那么问题来了。诸位看官可以思考一下 ① 测试代码不是同源同管理会有什么弊端 ② 长期存在多个分支、没有限制更新与合入时间会有什么弊端。 肆&nbsp; &nbsp; &nbsp; &nbsp; 大舒一口气，从分支角度看，至少我们的需求大部分并不是真的要开发数月才能上线，而是有机会做到小步快跑的？那么「存在周期中等长度」的分支又是什么情况呢。 开发周期大于10天的需求依然不少，是否因为需求拆解不够小？或是开发框架组件化解耦得不够细？还是自动化测试的基建不够好呢？ 除此之外还暴露出了另一个问题，分支的命名格式也太多样了吧。特别是经历过项目交接之后，不同团队中的不同个人都以各自的习惯提交？更不用说git commit的规范了，不方便回溯。 伍&nbsp; &nbsp; &nbsp; &nbsp; 存在周期长的分支问题暴露了这么多，最后剩下的「较短周期」的分支应该总没问题了吧？ 通过分支名统计了一下存在天数在十天以下的分支的用处，除了命名不规范没有被我统计到的以外，用于fix作用的分支与feature分支各占一半，比较符合预期。 三、分支管理策略&nbsp; &nbsp; &nbsp; &nbsp; 经过上述的简单统计后，大家可能只是对该案例项目的分支之多、生存周期之长有深刻印象。那么在EPC标准中，对于分支管理道理有什么要求，在《持续交付2.0》中对于分支管理又有什么建议呢。 3.1分支管理 - EPC等级&nbsp; &nbsp; &nbsp; &nbsp; 将[EPC 1.0]要求列出来后可以发现，在分支管理这个维度上，最终目标（Level 4）倒也算清晰简洁；除了要求分支要统一的命名规范外，量化指标就俩： *1. 分支存在的周期要短，80%的分支&lt;=2天， * 2. 每次提交的内容要少，80%的提交 &lt; 200行代码（除客户端UI的布局文件和相关配置文件外）。 &nbsp; &nbsp; &nbsp; &nbsp; 当然，要在业务需求的开发过程中实现这两点目标，却不是一件容易的事情。 3.2持续交付 - 分支管理模式&nbsp; &nbsp; &nbsp; &nbsp; 除了上述提及的两个分数要求外，大家应该也注意到了「分支开发，主干集成」的模式，悄悄地转变为了「主干开发，分支集成」的模式。&nbsp; &nbsp; &nbsp; &nbsp; 而最理想化的状态竟是「主干开发，主干集成」。一开始看到这个理念我也很不解，一旦抛弃了分支的存在，强大如Git所拥有的必杀技不就被禁足，一日被打回原型，与其他版本控制系统无异了吗？ &nbsp; &nbsp; &nbsp; &nbsp; 且慢，这个「主干开发，主干集成」的理想国故事，还要从「主干开发，主干集成」的伊甸园开始说起。 3.2.1 初级「主干开发，主干集成」 &nbsp; &nbsp; &nbsp; &nbsp; 摘录自乔老师在《持续集成2.0》中的描述。很好理解，其实个人开发或是2、3个人的小团队，需求不多时间不紧时，往往大家就是这样直接在master上修改，git提供的就是纯粹的代码备份服务。 &nbsp; &nbsp; &nbsp; &nbsp; 如果master代码处于长期不可用状态，只有等到所有功能开发完后才进行联调和集成测试，这就是「低频交付」的模式。 3.2.2 「分支开发，主干集成」 &nbsp; &nbsp; &nbsp; &nbsp; 当一个发布周期中的需求逐渐多了起来，需要合作的开发同学越来越多，新老同学的技术水平参差不齐的时候，就会发现Git的分支模型非常稳妥地提供了一种并行开发的解决方案，安全有保障、协作无干扰，这也是「特性分支模式」为什么备受青睐，广为业务团队所接受的原因（之一）。 &nbsp; &nbsp; &nbsp; &nbsp; 若是更大规模的团队（40人以上）共同开发一款产品，就更倾向于运用如上的分支开发模式。 &nbsp; &nbsp; &nbsp; &nbsp; 总的来说这个模式挺好的，唯一的弊端可能出现在合并的时候，一旦多个需求的修改有冲突，就会比较费神。&nbsp; &nbsp; &nbsp; &nbsp; 若要成功使用这种模式，其关键点在于： *1. 让主干尽可能一直保持着在可发布状态； * *2. 每个分支的生命周期尽可能短； * *3. 主干代码尽早与分支同步； * 4. 一切以主干代码为准，尽可能不要在各特性分支之间合并代码。 &nbsp; &nbsp; &nbsp; &nbsp; 但是分支开发模式，其实从本质上就是与持续集成的理念相互冲突的。持续集成是希望每次修改都尽早的提交到主干，主干总是处于最完整和最新的可用状态，充分验证后就可以用它来进行生产部署。而使用分支开发模式时，由于无法及时合并到主干，那么时间越长与主干差别越大，风险就越高，最终合并的时候就越痛苦。所以持续交付不推荐使用分支开发的模式[Ref. 3]。 3.2.3 「主干开发，分支集成」&nbsp; &nbsp; &nbsp; &nbsp; 来到发布前的集成测试节点了，功能已经全部开发完毕，通常这时候客户端团队就会从代码中拉出「发布」分支。 &nbsp; &nbsp; &nbsp; &nbsp; 模型中所示的“质量打磨周期”（Branch Stabilization Time）越短，说明主干质量越好，当打磨周期极短时，就可以转换到高频的「主干开发，主干集成」模式。 3.2.4 高级「主干开发，主干集成」 &nbsp; &nbsp; &nbsp; &nbsp; 是的，与3.2.1是同一张图，但是看到这里，相信大家应当对为何「主干开发，主干集成」是最低级的模式同时又是最高级的模式有所理解了。相比于「低频交付」的状态，「高频交付」状态下对代码质量的要求相当高，几乎是落子无bug的境界（笑）。&nbsp; &nbsp; &nbsp; &nbsp; 需要进行版本控制的不仅是源代码，还有测试代码、数据库脚本、构建和部署脚本、依赖的库文件等，并且对构建产物的版本控制也同样重要。只有这些内容都纳入版本控制了，才能够确保所有的开发、测试、运维活动能够正常开展，系统能够被完整的搭建。持续交付建议的方式是频繁的提交代码，并且最好工作在主干上，这样一来修改对所有项目成员都快速可见，然后通过持续集成的机制，对修改触发快速的自动化验证和反馈，再往后如果能通过各种维度的验证测试，最终将成为潜在可发布和部署到生产环境的中版本[Ref. 3]。&nbsp; &nbsp; &nbsp; &nbsp; 深入了解持续交付中对于分支管理的要求或者说期许之后，希望没有打击到大家的信心 / 希望反而能激起大家的技术追求。那么下一篇章就来谈谈一些近期搜刮到的/实用的/接地气的辅助方案了。 四、细节建议方案&nbsp; &nbsp; &nbsp; &nbsp;个人认为， 上述提到的向「主干开发，主干集成」靠拢的模式思路更适用于发布频率固定且快、质量要求高的处于快速生长期的客户端（e.g. iOS、Android）/类客户端（e.g. 小程序）产品。不同的产品形态、不同的产品周期还是要因地制宜地选择适合当前发展状态的分支管理模式。比如对于嵌入在APP不同位置的H5页面，因为相互独立故而可以选择建立分别的仓库直接采用主干开发的方式。&nbsp; &nbsp; &nbsp; &nbsp;有些场景下，迫不得已要采用分支开发的模式，比如并行需求太多且相互干扰，或者在需求开发的同时有大块的重构工作要做，或者针对特定的用户开发特殊的功能，以及需要进行与主线无关的试验等等。这时拉出分支其实意味着已经在持续集成/持续交付上做出了妥协，那么我们建议至少要使用一些折中的方案[Ref. 3]： 尽量缩短分支的周期，最长也不要超过迭代周期； 每个分支上运行单独的测试流水线，保证质量。虽然这种方式浪费资源，而且其实也没进行”真正的“集成； 分支只与主干合并代码，分支彼此之间尽量不做合并； 分支定期合并主干上的变更； 【重点】针对问题项目的分支管理改进&nbsp; &nbsp; &nbsp; &nbsp;针对第二节中分析的典型项目，我想可以有如下的改进方式： 按照上图所示的分支模式进行管理，除了主干与发布分支以外，其他不必要的分支均删去，减少与主干产生大差异的机会； 测试代码、数据库脚本、构建和部署脚本、依赖的库文件等等合入主干与源代码同源管理； 分支与需求绑定起来，使得每一次的修改有据可循（可参考4.1）； 约定特性分支的命名规范（比如feat_20200229_market，表示该特性分支最多存在到二月29日就要删去，功能是market），可以通过插件约束不规范分支的提交（可参考4.3）； 遵循git commit的提交信息规范，限制不合规范的messages的提交（可参考4.2）； 除非特殊需要，所有特性分支的存在周期都尽量压缩到五天以内；持续暗示自己分支不过周末。 4.1 分支与需求单&nbsp; &nbsp; &nbsp; &nbsp; 在3.2.2的模式中，每一个特性分支的创建都是为需求服务的。要想达到每个分支都在很短时间内消失的目标，不可否认前提条件是产品对需求的拆解和开发对代码的解耦都具备很高的能力，这是值得另开篇章阐述的话题，此处先留个#href的坑。&nbsp; &nbsp; &nbsp; &nbsp; 为了解决当前分支凌乱的问题，有一种办法是在需求单转入开发中时自动创建分支，git commit时提交关键字与需求ID绑定起来，不仅可以追溯每一次代码的变更都为了达成什么目的，划分模块责任人，更可以在git push的同时一键转单评论，繁琐的流程和鼠标点点点操作通通不存在。&nbsp; &nbsp; &nbsp; &nbsp; 参考《TAPD（腾讯敏捷产品研发平台）-工蜂Git关联新特性》，只需三步，轻松上手。 TAPD项目下启用「源码」应用，应用设置中关联相应的GIT仓库。 需求单与GIT分支关联。 Smart Commit关键字提交时触发状态流转。 效果示例： 12$ git commit -m \"fix: test --story=857210425 TAPD vs GIT #finish\"$ git push &nbsp; &nbsp; &nbsp; &nbsp; 分支关联需求单，commit关联需求提测 / bug解决，状态流转一步到位。 4.2. 代码提交规范&nbsp; &nbsp; &nbsp; &nbsp;编写规范良好的commit messages的优点无需赘言。不仅提高后续code review的效率，帮助团队成员清晰地了解到每一次提交修改了什么特性，也方便了版本日志的生成。&nbsp; &nbsp; &nbsp; &nbsp;业界最常用的Angular规范如下（只展示Header要求）： &lt;type&gt;[optional scope]: &lt;description&gt;&nbsp; &nbsp; &nbsp; &nbsp; Header部分只有一行，其中&lt;type&gt;用于说明 commit 的类别，只允许使用下面7个标识： feat：新功能（feature） fix：修补bug docs：文档（documentation） style： 格式（不影响代码运行的变动） refactor：重构（即不是新增功能，也不是修改bug的代码变动） test：增加测试 chore：构建过程或辅助工具的变动&nbsp; &nbsp; &nbsp; &nbsp; &lt;scope&gt;用于说明 commit 影响的范围，比如数据层、控制层、视图层等等，视项目不同而不同。是可选项。&nbsp; &nbsp; &nbsp; &nbsp; &lt;description&gt;是 commit 目的的简短描述，不超过50个字符。 4.3. 辅助工具&nbsp; &nbsp; &nbsp; &nbsp; 无论是何种规范的制定，最终目的都是服务于研发流程的提效，如果它的存在阻碍了开发的丝滑进行，那它就不是一个好的规范。&nbsp; &nbsp; &nbsp; &nbsp; 所以我们需要一些自动化的提示和约束，来方便标准化规范的落实。 4.3.1 分支命名规范&nbsp; &nbsp; &nbsp; &nbsp; 参考《Feflow在CI中检查项目Git规范》提供的前端方案，feflow-plugin-check插件（后续可能会对外开源：https://github.com/iv-web/feflow）: * 分支版本命名规则：分支类型_分支发布时间_分支功能。比如：feature_20170401_fairy_flower * 分支类型包括：feature、 bugfix、refactor三种类型，即新功能开发、bug修复和代码重构 * 时间使用年月日进行命名，不足2位补0 * 分支功能命名使用snake case命名法，即下划线命名。效果示例： 4.3.2 关联tapd并规范commit参考《优雅提交git commit并强制关联tapd需求单》提供的方案（后续可能会对外开源），只需两步：（1）添加package.json 和 commitlint.config.js 文件。（2）执行tnpm install 效果示例：a) 没有输入正确的tapd关联b) 没有输入正确的Angular规范c) 符合要求的commit message 结语&nbsp; &nbsp; &nbsp; &nbsp; 良好的分支管理只是实现持续交付持续部署的其中一个必不可少的环节，要让DevOps更丝滑，研发流程效率更高，还需要产品-开发-测试同学更加专业化、更加规范化地一同努力。 附录持续交付中的分支管理Ref. 1 乔梁-《持续交付2.0业务引领的DevOps精要》 Ref. 2 {内部}持续交付体系设计与实践 - 工程效率提升之路 Ref. 3 百度资深敏捷教练：深度解析持续交付之全面配置管理 Ref. 4 化繁为简的企业级 Git 管理实战（三）：分支管理策略 Git分支规范好工具Ref. 5 {内部}Git commit message和工作流规范 Ref. 6 {内部}优雅提交git commit并强制关联tapd需求单 Ref. 7 Git Commit 规范 | Feflow Ref. 8 {内部}使用Feflow在CI中检查项目Git规范","categories":[{"name":"DevOps","slug":"DevOps","permalink":"icyhh.xyz/categories/DevOps/"}],"tags":[{"name":"DevOps","slug":"DevOps","permalink":"icyhh.xyz/tags/DevOps/"}]},{"title":"Lecture0x10_SVM","slug":"Lecture0x10-SVM","date":"2020-05-04T02:45:44.000Z","updated":"2020-09-23T08:41:04.957Z","comments":true,"path":"2020/05/04/Lecture0x10-SVM/","link":"","permalink":"icyhh.xyz/2020/05/04/Lecture0x10-SVM/","excerpt":"","text":"继续发一个身份卡。 名称 中文 分类模型 回归模型 关注参数 适用场景 必备技巧 Support Vector Machine z Yes Yes 核函数，误差C。 特征维度小，非线性可分。 特征归一化。 本周持续dancing &amp; GRE learning，本文还有补充空间~ 附录Ref. 1 入门支持向量机1：图文详解SVM原理与模型数学推导 Ref. 2 入门支持向量机2：软间隔与sklearn中的SVM Ref. 3 入门支持向量机3：巧妙的Kernel Trick Ref. 4 入门支持向量机4：多项式核函数与RBF核函数代码实现及调参 Ref. 5 入门支持向量机5：回归问题及系列回顾总结","categories":[],"tags":[]},{"title":"Lecture0x09-PCA","slug":"Lecture0x09-PCA","date":"2020-04-26T13:19:50.000Z","updated":"2020-09-23T08:41:04.957Z","comments":true,"path":"2020/04/26/Lecture0x09-PCA/","link":"","permalink":"icyhh.xyz/2020/04/26/Lecture0x09-PCA/","excerpt":"","text":"本周补班+GRE，PCA嘛再说啦~ 附录Ref. 1 数据降维1：主成分分析法思想及原理 Ref. 2 数据降维2：PCA算法的实现及使用 Ref. 3 数据降维3：降维映射及PCA的实现与使用 Ref. 4 数据降维之应用：降噪&amp;人脸识别","categories":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"icyhh.xyz/categories/Machine-Learning/"}],"tags":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"icyhh.xyz/tags/Machine-Learning/"}]},{"title":"Lecture0x08_DecisionTree","slug":"Lecture0x08-DecisionTree","date":"2020-04-19T15:37:32.000Z","updated":"2020-09-23T08:41:04.957Z","comments":true,"path":"2020/04/19/Lecture0x08-DecisionTree/","link":"","permalink":"icyhh.xyz/2020/04/19/Lecture0x08-DecisionTree/","excerpt":"","text":"发一个身份卡。 名称 中文 分类模型 回归模型 关注参数 适用场景 必备技巧 Decision Tree 决策树 Yes Yes 划分阈值的选择 要求有较好的解释性的场景。 特征离散化、剪枝。 本周努力学习GRE，本文还有补充空间~ 附录Ref. 1 决策树1：初识决策树 Ref. 2 决策树2: 特征选择中的相关概念 Ref. 3 决策树3: 特征选择之寻找最优划分 Ref. 4 决策树4：构建算法之ID3、C4.5 Ref. 5 决策树5：剪枝与sklearn中的决策树 Ref. 6 决策树6：分类与回归树CART","categories":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"icyhh.xyz/categories/Machine-Learning/"}],"tags":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"icyhh.xyz/tags/Machine-Learning/"}]},{"title":"Lecture0x07_LogisticRegression","slug":"Lecture0x07-LogisticRegression","date":"2020-04-12T10:50:42.000Z","updated":"2020-09-23T08:41:04.957Z","comments":true,"path":"2020/04/12/Lecture0x07-LogisticRegression/","link":"","permalink":"icyhh.xyz/2020/04/12/Lecture0x07-LogisticRegression/","excerpt":"","text":"本周努力学习data analysis，本文留白~ 附录Ref. 1《出场率No.1的逻辑回归算法，是怎样“炼成”的？》 Ref. 2《逻辑回归的本质及其损失函数的推导、求解》 Ref. 3《逻辑回归代码实现与调用》 Ref. 4《逻辑回归的决策边界及多项式》 Ref.5 《sklearn中的逻辑回归中及正则化》","categories":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"icyhh.xyz/categories/Machine-Learning/"}],"tags":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"icyhh.xyz/tags/Machine-Learning/"}]},{"title":"Lecture0x06_pipeline","slug":"Lecture0x06-pipeline","date":"2020-04-06T04:14:31.000Z","updated":"2020-09-23T08:41:04.956Z","comments":true,"path":"2020/04/06/Lecture0x06-pipeline/","link":"","permalink":"icyhh.xyz/2020/04/06/Lecture0x06-pipeline/","excerpt":"","text":"本周努力刷leetcode，本文留白~ 附录Ref. 1 Japson / 《浅析多项式回归与sklearn中的Pipeline》 Ref.2 Japson / 《ML/DL重要基础概念：偏差和方差》 Ref.3 Japson /《（理论+代码）模型正则化：L1正则、L2正则》","categories":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"icyhh.xyz/categories/Machine-Learning/"}],"tags":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"icyhh.xyz/tags/Machine-Learning/"}]},{"title":"Lecture0x05_GradientDescent","slug":"Lecture0x05-GradientDescent","date":"2020-03-28T09:41:07.000Z","updated":"2020-09-23T08:41:04.956Z","comments":true,"path":"2020/03/28/Lecture0x05-GradientDescent/","link":"","permalink":"icyhh.xyz/2020/03/28/Lecture0x05-GradientDescent/","excerpt":"","text":"从机器学习方法论说起WHAT「机器学习」就是找到一种函数f(x)并进行优化， 完成预测、分类、生成等工作。 HOW第一步：定义一个函数集合（define a function set）第二步：判断函数的好坏（goodness of a function）第三步：选择最好的函数（pick the best one） HOW 3pick the best one?「梯度下降」是目前机器学习、深度学习解决最优化问题的算法中，最核心、应用最广的方法。 梯度是什么多元函数的导数(derivative)就是梯度(gradient)，分别对每个变量进行微分。梯度是向量(有方向)，其维度与参数的维度一样。梯度指向误差值增加最快的方向。 对于损失函数L，$f(x_1, x_2)$，梯度为$\\nabla L = (\\frac{\\partial L}{\\partial x_1},\\frac{\\partial L}{\\partial x_2})$ 为什么靠梯度下降从数学的角度出发，针对损失函数$L$，假设选取的初始点为$(a_0, b_0)$，将这个点稍微移动一点点，得到$(a_1, b_1)$。 移动一点点的增量 $\\Delta L = L(a_1, b_1) - L(a_0, b_0) \\approx \\frac{\\partial L}{\\partial a}\\Delta a + \\frac{\\partial L}{\\partial b}\\Delta b$ （代入了泰勒展开式） 如果令移动的距离$\\Delta a$、$\\Delta b$分别为$-\\eta\\frac{\\partial L}{\\partial a}$、$-\\eta\\frac{\\partial L}{\\partial b}$，其中规定$\\eta &gt; 0$，则： $\\Delta L \\approx -\\eta (\\frac{\\partial L}{\\partial a})^2 + (\\frac{\\partial L}{\\partial b})^2 \\leqslant 0$ 我们如果按照规定的移动距离公式移动参数，那么损失函数的函数值始终是下降的，这样就达到了我们要求的“损失变小”的要求了。如果一直重复这种移动，则可以证明损失函数最终能够达到一个最小值。 所以下一个点的迭代公式： $(a_{k+1}, b_{b+1})=(a_{k}+\\Delta a, b_{b}+\\Delta b)) = (a_{k}-\\eta\\frac{\\partial L}{\\partial a}, b_{b}-\\eta\\frac{\\partial L}{\\partial b}))$ 为什么梯度要乘以一个负号梯度的方向就是损失函数值在此点上升最快的方向，是损失增大的区域；而我们要使损失最小，因此就要逆着梯度方向走，自然就是负的梯度的方向。 关于参数$\\eta$我们已经知道，梯度对应的是下山的方向，而参数 对应的是步伐的长度。在学术上，我们称之为“学习率”(learning rate)，是模型训练时的一个很重要的超参数，能直接影响算法的正确性和效率： 学习率$\\eta$不能太大。从数学角度上来说，一阶泰勒公式只是一个近似的公式，只有在学习率很小，也就是$\\Delta a$, $\\Delta b$很小时才成立。直观上考虑，如果学习率太大，那么有可能会“迈过”最低点，从而发生“摇摆”的现象（不收敛），无法得到最低点。 学习率又不能太小。如果太小，会导致每次迭代时，参数几乎不变化，收敛学习速度变慢，使得算法的效率降低，需要很长时间才能达到最低点。 梯度下降算法的缺点从理论上，它只能保证达到局部最低点，而非全局最低点。在很多复杂函数中有很多极小值点，我们使用梯度下降法只能得到局部最优解，而不能得到全局最优解。 对应的解决方案有：首先随机产生多个初始参数集，即多组$a_{0}$, $b_{0}$；然后分别对每个初始参数集使用梯度下降法，直到函数值收敛于某个值；最后从这些值中找出最小值，这个找到的最小值被当作函数的最小值。当然这种方式不一定能找到全局最优解，但是起码能找到较好的。 如何实施梯度下降//todo Ref. 2&amp;3 优化梯度下降算法 名称 中文 特点 优缺点 BGD (Batch Gradient Descent) 批量梯度下降法 通过对整个数据集的所有样本的计算来求解梯度的方向。 在数据量很大时需要计算很久 SGD (Stochastic Gradient Descent） 随机梯度下降法 每次迭代使用一个样本来对参数进行更新。虽然不是每次迭代得到的损失函数都向着全局最优方向，但是大的整体的方向是向全局最优解的。学习率$\\eta$需要逐渐递减的。 相比于批量梯度，这样的方法更快。但随机梯度下降有着不可预知性。 SGDM (SGD with Momentum) 当前权值的改变会收到上一次权值的改变的影响，就像小球滚动时候一样，由于惯性，当前状态会受到上一个状态影响，这样可以加快速度。 AdaGrad (Adative Gradient Descent) 自适应梯度 核心思想是对于常见的数据给予比较小的学习率去调整参数，对于不常见的数据给予比较大的学习率调整参数。它可以自动调节学习率，但迭代次数多的时候，学习率也会下降。 加大小梯度方向的进步，自动调节学习率；但是时间太长的话，会造成步长太小，困在局部极值点。 RMSProp 均方差传播 基于权重梯度最近量级的均值为每一个参数适应性地保留学习率。这意味着算法在非稳态和在线问题上有很有优秀的性能。 Adam 它的名称来源于适应性矩估计（adaptive moment estimation） AdaGrad通过计算梯度的一阶矩估计和二阶矩估计而为不同的参数设计独立的自适应性学习率。 同时获得了 AdaGrad 和 RMSProp 算法的优点。 //to be continue. 附录Ref. 1 Japson / 还不了解梯度下降法？看完这篇就懂了！ Ref.2 Japson / 手动实现梯度下降（可视化） Ref.3 Japson / 线性回归中的梯度下降 Ref.4 Japson /速度更快的随机梯度下降法 Ref.5 Japson /梯度下降番外：非常有用的调试方式及总结","categories":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"icyhh.xyz/categories/Machine-Learning/"}],"tags":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"icyhh.xyz/tags/Machine-Learning/"}]},{"title":"Lecture0x04_LinearRegression","slug":"Lecture0x04-LinearRegression","date":"2020-03-21T16:21:13.000Z","updated":"2020-09-23T08:41:04.956Z","comments":true,"path":"2020/03/22/Lecture0x04-LinearRegression/","link":"","permalink":"icyhh.xyz/2020/03/22/Lecture0x04-LinearRegression/","excerpt":"","text":"线性回归发一个身份卡。 名称 中文 分类模型 回归模型 关注参数 适用场景 必备技巧 Linear Regression 线性回归 No Yes L1、L2范数 维数低，每一维之间都没有共线性 本周忙昏，本文留白~ 附录Ref. 1 Japson / 模型之母：简单线性回归&amp;最小二乘法 &amp; 特征工程系列：特征预处理（下） Ref.2 Japson / 模型之母：简单线性回归的代码实现 Ref.3 Japson / 模型之母：多元线性回归","categories":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"icyhh.xyz/categories/Machine-Learning/"}],"tags":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"icyhh.xyz/tags/Machine-Learning/"}]},{"title":"Lecture0x03_feature engineering","slug":"Lecture0x03-feature","date":"2020-03-11T11:25:33.000Z","updated":"2020-09-23T08:41:04.956Z","comments":true,"path":"2020/03/11/Lecture0x03-feature/","link":"","permalink":"icyhh.xyz/2020/03/11/Lecture0x03-feature/","excerpt":"","text":"数据预处理与特征工程 思维导图直达：https://mubu.com/doc/5rWv4i8HggF Data PreProcessing（数据预处理）1. 无量纲化 名称 适用场景 做法 公式 特点 缺点 标准化(Standardization) 1. 某些算法要求样本具有零均值和单位方差； 2.需要消除样本不同属性具有不同量级时的影响； 3.在分类、聚类算法中，需要使用距离来度量相似性的时候（如SVM、KNN），或者使用PCA技术进行降维的时候，Z-score表现更好。 基于原始数据的均值（mean）和标准差（standard deviation）进行数据的标准化。 $$x’ = \\frac{x-\\mu}{\\sigma} $$其中 $\\mu = \\frac{1}{N} \\sum_{i=1}^{N}x_i$ $\\sigma = \\sqrt{\\frac{1}{N}\\sum_{i=1}^{N}}$ 简单，容易计算，Z-Score能够应用于数值型的数据，并且不受数据量级的影响。标准化只是调整特征整体的分布。 1.对于数据的分布有一定的要求，正态分布是最有利于Z-Score计算的 2.Z-Score消除了数据具有的实际意义，因此Z-Score的结果只能用于比较数据间的结果，关注数据的真实意义还需要还原其原值 3.如果存在异常值，则无法保证平衡的特征尺度。 MinMax归一化(Normalization) 1.归一化有可能提高精度——数量级的差异将导致量级较大的属性占据主导地位，但实际情况有可能是值域范围小的特征更重要； 区间缩放法利用了边界值信息，将属性缩放到[0,1]。 $$x’ = \\frac{x-Min}{Max-Min} $$ 归一化输出在[0,1]或[-1,1]之间；如果数据较为稳定，不存在极端的最大最小值，用归一化。 1.当有新数据加入时，可能导致max和min的变化，需要重新定义； MaxAbs归一化(Normalization) 2.数量级的差异将导致迭代收敛速度减慢——当使用梯度下降法寻求最优解时，很有可能走“之字型”路线（垂直等高线走），从而导致需要迭代很多次才能收敛；3.依赖于样本距离的算法对于数据的数量级非常敏感，务必归一化。 单独地缩放和转换每个特征，使得训练集中的每个特征的最大绝对值将为1.0，将属性缩放到[-1,1]。它不会移动/居中数据，因此不会破坏任何稀疏性。 $$x’ = \\frac{x}{ Max } $$ 正态分布化 1.将每个样本缩放到单位范数(每个样本的范数为1)，如果要使用如二次型(点积)或者其它核方法计算两个样本之间的相似性，这个方法会很有用。2.该方法是文本分类和聚类分析中经常使用的——向量空间模型（Vector Space Model)的——基础。 对每个样本计算其$L_{p}$范数，然后对该样本中每个元素除以该范数，这样处理的结果是使得每个处理后样本的$L_{p}$ 范数(L1-norm, L2-norm)等于1。 $$x’ = \\frac{x}{L_{2}-norm } $$ $L_{p}$范数： 在线性代数，函数分析等数学分支中，范数（Norm）是一个函数，其赋予某个向量空间（或矩阵）中的每个向量以长度或大小。 $L_{0}-norm$ $||w||{0} = num(i)$ $with$ $ x{i} \\neq 0 $ （表示向量中所有非零元素的个数） $L_{1}-norm$ $$||w||{1} = \\sum{1}^{d}|x_i| $$ （表示每个元素的绝对值之和） $L_{2}-norm$ $$||w||{2} = \\sqrt{\\sum{i=1}^{d}(x_j)^2} $$ （欧氏距离） $L_{p}-norm$ $$||w||{p} = ({\\sum{i=1}^{d}x_j^p})^{\\frac{1}{p}} $$ $L_{max}-norm$ （对象属性之间的最大距离，） $$||w||_{\\infty} = max（│x_1│，│x_2│，…，│x_n│）$$ 可以看到sklearn中对「正态分布化」的实现细节： 12345678if norm &#x3D;&#x3D; &#39;l1&#39;: norms &#x3D; np.abs(X).sum(axis&#x3D;1)elif norm &#x3D;&#x3D; &#39;l2&#39;: norms &#x3D; row_norms(X)elif norm &#x3D;&#x3D; &#39;max&#39;: norms &#x3D; np.max(X, axis&#x3D;1)norms &#x3D; _handle_zeros_in_scale(norms, copy&#x3D;False)X &#x2F;&#x3D; norms[:, np.newaxis] 总结 一般来说，建议优先使用标准化。对于输出有要求时再尝试别的方法，很多方法都可以将输出范围调整到[0, 1]；如果我们对于数据的分布有假设的话，更加有效的方法是使用相对应的概率密度函数来转换。 如果数据存在异常值和较多噪音，用标准化，可以间接通过中心化避免异常值和极端值的影响。 除了上面介绍的方法外，还有一些相对没这么常用的处理方法：RobustScaler、PowerTransformer、QuantileTransformer和QuantileTransformer等。 2. 特征分桶——即数据离散化，将数值型特征特征分箱。 适用场景： 离散化后的特征对异常数据有很强的鲁棒性（过分远离的特征也可落入其中一个分类）； 对于线性模型，表达能力受限；单变量离散化为N个后，每个变量有单独的权重，相当于模型引入了非线性，能够提升模型表达能力，加大拟合； 离散化后可以进行特征交叉，由M+N个变量变为M*N个变量，进一步引入非线性，提升表达能力； 特征离散化后，模型会更稳定（比如如果对用户年龄离散化，20-30作为一个区间，不会因为一个用户年龄长了一岁就变成一个完全不同的人；当然处于区间相邻处的样本会刚好相反，所以怎么划分区间是门学问）； 特征离散化以后，起到了简化了逻辑回归模型的作用，降低了模型过拟合的风险； 可以将缺失作为独立的一类带入模型； 将所有变量变换到相似的尺度上。 无监督分箱法 名称 做法 特点 等距分箱 按照相同宽度将数据分成几等份。 受到异常值的影响比较大。 等频分享 将数据分成几等份，每等份数据里面的个数是一样的。 聚类分箱 基于k均值聚类的分箱：k均值聚类法将观测值聚为k类，但在聚类过程中需要保证分箱的有序性：第一个分箱中所有观测值都要小于第二个分箱中的观测值，第二个分箱中所有观测值都要小于第三个分箱中的观测值，等等。 二值化 二值化可以将数值型（numerical）的feature进行阀值化得到boolean型数据。这对于下游的概率估计来说可能很有用（比如：数据分布为Bernoulli分布时）。 $$ x’ = \\left{ \\begin{aligned} 1, &amp; &amp; x&gt;threshold \\ 0, &amp; &amp; x \\leq threshold \\end{aligned} \\right.{ }$$ 有监督分箱法 名称 做法 主要思想 卡方分箱法 自底向上的（基于合并的）数据离散化方法。它依赖于卡方检验：具有最小卡方值的相邻区间合并在一起，直到满足确定的停止准则。 对于精确的离散化，相对类频率在一个区间内应当完全一致。因此,如果两个相邻的区间具有非常类似的类分布，则这两个区间可以合并；否则，它们应当保持分开。而低卡方值表明它们具有相似的类分布。 最小熵法分箱 分箱使总熵值达到最小，也就是能够最大限度地区分因变量的各类别（类间差异最大，类内差异最小）。 熵是信息论中数据无序程度的度量标准，提出信息熵的基本目的是找出某种符号系统的信息量和冗余度之间的关系，以便能用最小的成本和消耗来实现最高效率的数据存储、管理和传递。 总结 我们对特征进行分箱后，需要对分箱后的每组（箱）进行woe编码和IV值的计算，通过IV值进行变量筛选后，然后才能放进模型训练。 分箱后需要进行特征编码，如：LabelEncode、OneHotEncode或LabelBinarizer等。 3. 统计变换 数据分布的倾斜有很多负面的影响。我们可以使用特征工程技巧，利用统计或数学变换来减轻数据分布倾斜的影响。使原本密集的区间的值尽可能的分散，原本分散的区间的值尽量的聚合。 这些变换函数都属于幂变换函数簇，通常用来创建单调的数据变换。它们的主要作用在于它能帮助稳定方差，始终保持分布接近于正态分布并使得数据与分布的平均值无关。 名称 作用 公式 Log变换 Log变换通常用来创建单调的数据变换。 $$y = log_b{x}$$ Box-Cox变换 Box-Cox变换的主要特点是引入一个参数，通过数据本身估计该参数进而确定应采取的数据变换形式。 $$ f(x, \\lambda) = x^\\lambda \\left{ \\begin{aligned} \\frac{x^\\lambda - 1}{\\lambda}, &amp; &amp; for \\lambda &gt; 0 \\ log_e(x), &amp; &amp; for \\lambda = 0 \\end{aligned} \\right.{ }$$λ 的最佳取值通常由最大似然或最大对数似然确定。 Log变换 它的主要作用在于帮助稳定方差，始终保持分布接近于正态分布并使得数据与分布的平均值无关。Log变换倾向于拉伸那些落在较低的幅度范围内自变量值的范围，倾向于压缩或减少更高幅度范围内的自变量值的范围。从而使得倾斜分布尽可能的接近正态分布。 适用场景： 针对一些数值连续特征的方差不稳定，特征值重尾分布我们需要采用Log化来调整整个数据分布的方差，属于方差稳定型数据转换。比如在词频统计中，有些介词的出现数量远远高于其他词，这种词频分布的特征就会现有些词频特征值极不协调的状况，拉大了整个数据分布的方差。这个时候，可以考虑Log化。尤其在分本分析领域，时间序列分析领域，Log化非常常见, 其目标是让方差稳定，把目标关注在其波动之上。 Box-Cox变换 Box-Cox变换是Box和Cox在1964年提出的一种广义幂变换方法，是统计建模中常用的一种数据变换，用于连续的响应变量不满足正态分布的情况。 该函数有一个前提条件，即数值型值必须先变换为正数（与 log 变换所要求的一样）。万一出现数值是负的，使用一个常数对数值进行偏移是有帮助的。 适用场景： Box-Cox变换之后，可以一定程度上减小不可观测的误差和预测变量的相关性。Box-Cox变换可以明显地改善数据的正态性、对称性和方差相等性，对许多实际数据都是行之有效的。 4. 特征编码 在统计学中，分类特征是可以采用有限且通常固定数量的可能值之一的变量，基于某些定性属性将每个个体或其他观察单元分配给特定组或名义类别。 名称 定义 优点 缺点 标签编码（LabelEncode） 对不连续的数字或者文本进行编号，编码值介于0和n_classes-1之间的标签。 占用内存空间小，并且支持文本特征编码。 它隐含了一个假设：不同的类别之间，存在一种顺序关系。在具体的代码实现里，LabelEncoder会对定性特征列中的所有独特数据进行一次排序，从而得出从原始输入到整数的映射。所以目前还没有发现标签编码的广泛使用，一般在树模型中可以使用。 独热编码（OneHotEncode） 用于将表示分类的数据扩维。 因为大部分算法是基于向量空间中的度量来进行计算的，用独热编码能使非偏序关系的变量取值不具有偏序性，并且到圆点是等距的。 1.只能对数值型变量二值化，无法直接对字符串型的类别变量编码。2.当类别的数量很多时，特征空间会变得非常大。在这种情况下，一般可以用PCA来减少维度。而且one hot encoding+PCA这种组合在实际中也非常有用。 平均数编码（mean encoding） 如果某一个特征是定性的（categorical），而这个特征的可能值非常多（高基数），那么平均数编码（mean encoding）是一种高效的编码方式。在实际应用中，这类特征工程能极大提升模型的性能。 总结 由于树模型（Random Forest、GBDT、xgboost等）对特征数值幅度不敏感，可以不进行无量纲化和统计变换处理； 依赖样本距离来学习的模型（如线性回归、SVM、深度学习等） 对于数值型特征需要进行无量纲化处理； 对于一些长尾分布的数据特征，可以做统计变换，使得模型能更好优化； 对于线性模型，特征分箱可以提升模型表达能力； 对数值型特征进行特征分箱可以让模型对异常数据有很强的鲁棒性，模型也会更稳定；分箱后也需要进行特征编码。 Feature Extraction（特征提取）Feature Selection（特征选择）Feature construction（特征构造）附录Ref. 1 木东居士 / ​特征工程系列：特征预处理（上） &amp; 特征工程系列：特征预处理（下） Ref. 2 木东居士 / 机器学习的敲门砖：归一化与KD树 Ref. 3 是安酱和菜菜呀 / sklearn中的数据预处理和特征工程 Ref.4 Dave / ML 入门：归一化、标准化和正则化","categories":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"icyhh.xyz/categories/Machine-Learning/"}],"tags":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"icyhh.xyz/tags/Machine-Learning/"}]},{"title":"Lecture0x02_measurement","slug":"Lecture0x02-measurement","date":"2020-03-02T15:52:03.000Z","updated":"2020-09-23T08:41:04.955Z","comments":true,"path":"2020/03/02/Lecture0x02-measurement/","link":"","permalink":"icyhh.xyz/2020/03/02/Lecture0x02-measurement/","excerpt":"","text":"1. 回归模型中的指标选择评判指标一览 在预测任务中，给定样例集$D = {(x_1, y_1), (x_2, y_2),…,(x_n,y_n)}$，其中$y_i$是样本$x_i$的真实的观测值，$n$表示样本数量，$f(x_i)$代表对样本$x_i$的预测值。 名称 中文 特点 公式 优劣 Mean Absolute Error（MAE） 平均绝对误差 $$[0, +\\infty)$$误差越小越好。也即L1损失。 $$MAE =\\frac{1}{n}\\sum_{i=0}^{n}{ y_i - f(x_i) Mean Absolute Pencentage Error (MAPE） 平均绝对百分比误差 $$[0, +\\infty)$$误差越小越好。加权版的MAE $$MAPE = \\frac{1}{n}\\sum_{i=0}^{n} \\frac{y_i - f(x_i)}{y_i} Mean Squared Error/Deviation（MSE/D） 均方误差 $$[0, +\\infty)$$误差越小越好。也即L2损失。 $$MSE = \\frac{1}{n}\\sum_{i=0}^{n}(f(x_i)-y_i)^2$$ 更易受到极端值影响 Root Mean Square Error/Deviation（RMSE/D） 均方根误差 $$[0, +\\infty)$$误差越小越好。MSE开根号 $$RMSE = \\sqrt{\\frac{1}{n}\\sum_{i=0}^{n}(f(x_i)-y_i)^2}$$ 容易受到极端值影响 Normalized Root Mean Square Error（NRMSE） 归一化的均方根误差 $$(-\\infty, 1]$$误差越小越好。归一化的RMSE $$NRMSE = \\frac{RMSE}{y_{max}-y_{min}}$$ 容易受到极端值影响 Coefficient of determination （R squared） 决定系数 $$[0, 1]$$反映因变量的全部变异能通过回归关系被自变量解释的比例，越大越好。 $$R^2=1-1 - \\frac{\\sum\\limits_i(y_i - f(x_i))^2 }{\\sum\\limits_i(y_i - \\bar{y})^2}$$ 考虑了预测值与真值之间的差异&amp;问题本身真值之间的差异；归一化的度量标准 关于$R^2$的深入了解$R^2$的公式推导 搞清楚R2_score计算之前，我们还需要了解几个统计学概念。若用$y_i$表示真实的观测值，用$\\bar{y}$表示真实观测值的平均值，用$\\hat{y_i}$表示预测值,则： 1、回归平方和：SSR $$SSR = \\sum_{i=1}^{n}(\\hat{y_i} - \\bar{y})^2$$ 即估计值与平均值的误差，反映自变量与因变量之间的相关程度的偏差平方和。 2、残差平方和：SSE $$SSE = \\sum_{i=1}^{n}(y_i-\\hat{y_i} )^2$$ 即估计值与真实值的误差，反映模型拟合程度。 3、总离差平方和：SST$$SST =SSR + SSE= \\sum_{i=1}^{n}(y_i - \\bar{y})^2$$ 即平均值与真实值的误差，反映与数学期望的偏离程度。 $$R^2=1-\\frac{SSE}{SST} = 1 - \\frac{\\sum\\limits_i(y_i - \\hat{y_i})^2 / n}{\\sum\\limits_i(y_i - \\bar{y})^2 / n} = 1 - \\frac{MSE}{Var}$$分子就变成了常用的评价指标均方误差MSE，分母就变成了方差。 $R^2$的理解 对于$R^2$可以通俗地理解为使用均值作为误差基准，看预测误差是否大于或者小于均值基准误差。 R2_score = 1，样本中预测值和真实值完全相等，没有任何误差，表示回归分析中自变量对因变量的解释越好，也表示我们的预测模型不犯任何错误。 R2_score = 0。此时分子等于分母，样本的每项预测值都等于均值。 如果R2_score &lt; 0，说明我们学习到的模型还不如基准模型（样本均值）。很有可能我们的数据不存在任何线性关系。 R2_score不是R的平方，也可能为负数(分子&gt;分母)，模型等于盲猜，还不如直接计算目标变量的平均值。 $R^2$的运用： 1、$R^2$ 一般用在线性模型中（非线性模型也可以用） 2、$R^2$不能完全反映模型预测能力的高低，假如某个实际观测的自变量取值范围很窄，此时所建模型的$R^2$ 会较大，但这并不代表模型在外推应用时的效果肯定会很好。 3、数据集的样本越大，$R^2$越大，因此，不同数据集的模型结果比较会有一定的误差，此时可以使用Adjusted R-Square (校正决定系数），能对添加的非显著变量给出惩罚: $R^2_{\\text{Adj}}=1-(1-R^2)\\frac{n-p-1}{n-1}$ ，其中n是样本的个数，p是变量的个数 其他注意事项 在选用评价指标时，需要考虑： 真实观测值数据中是否有0 ，如果有0值就不能用MPE、MAPE之类的指标； 数据的分布如何 ，如果是长尾分布可以选择带对数变换的指标，中位数指标比平均数指标更好； 是否存在极端值 ，诸如MAE、MSE、RMSE之类容易受到极端值影响的指标就不要选用； 对异常值而言，中位数比均值更加鲁棒，MAE比MSE更不易受到离群值影响；但MAE存在一个严重问题，绝对值函数不是处处可导的，更新梯度始终相同，这不利于模型的学习，因此确定损失函数时不能用MAE。但是在评价模型时不影响，可以用。因此模型的评价方法可以和损失函数不同。 得到的指标是否依赖于量纲 (即绝对度量，而不是相对度量)，如果指标依赖量纲那么不同模型之间可能因为量纲不同而无法比较；（需要归一化） 平方操作会放大样本中预测结果和真实结果较大的差距。MAE没有放大。而我们就是要解决目标函数最大差距，因为选RMSE更好一点。 2. 分类模型中的指标选择混淆矩阵（confusion_matrix） 预测值_反例0 预测值_正例1 真实值_反例0 True Negative（TN） False Positive（FP） 真实值_正例1 False Negative（FN） True Positive（TP） 指标accuracy、precision、recall、F1 名称 中文 公式 定义/用法 特点 accuracy 准确度 $$Accuracy = \\frac{TP+TN}{TN+FP+FN+TP}$$ 所有样本中预测对的比率 对于原本就有偏（正反例不平衡）的样本集，准确度无法说明问题。 precision 查准率 $$Precision = \\frac{TP}{TP+FP}$$ 所有预测为正例的样本中真的为正例的比率 倾向于只挑选最有把握的样例时，查准率较高。 recall 查全率/召回率 $$ Recall = \\frac{TP}{TP+FN}$$ 所有真实正例中被预测对的比率 倾向于预测为1时，查全率较高。 F1-score - $$\\frac{1}{F1}=\\frac {1} {Precision}+\\frac{1}{Recall}$$$$F1=\\frac {2PR} {P+R}$$ 基于precision与recall的调和平均 Precision与Recall相互矛盾；而F1越高，模型越稳健。 P-R曲线 在P-R曲线中，Recall为横坐标，Precision为纵坐标。在P-R曲线中，曲线越凸向右上角越好。 P-R曲线的生成方法：算法对样本进行分类时，都会有置信度，即表示该样本是正样本的概率，比如99%的概率认为样本Ａ是正例，１％的概率认为样本B是正例。通过选择合适的阈值，比如50%，对样本进行划分，概率大于50%的就认为是正例，小于50%的就是负例。通过置信度就可以对所有样本进行排序，再逐个样本的选择阈值，在该样本之前的都属于正例，该样本之后的都属于负例。每一个样本作为划分阈值时，都可以计算对应的precision和recall，那么就可以以此绘制曲线。 （配图从Lecture0x01中生成） Recall值是递增的（但并非严格递增），随着划分点左移，正例被判别为正例的越来越多，不会减少。而精确率precision并非递减，二是有可能振荡的，虽然正例被判为正例的变多，但负例被判为正例的也变多了，因此precision会振荡，但整体趋势是下降。 P-R曲线肯定会经过（0, 1）点，比如将所有的样本全部判为负例，此时FP=0，TP=0，FN=正样本数，TN=负样本数，则（如果此处推断有误欢迎指出）$$Precision = \\frac{TP}{TP+FP} = \\frac{TP}{TP} \\rightarrow 1 $$ $$ Recall = \\frac{TP}{TP+FN} = 0 $$ 随着阈值点左移（变小），将所有的样本全部判为正例时，FN=0，TN=0，FP=负样本数，TP=正样本数，故而有$$ Recall = \\frac{TP}{TP+FN} = \\frac{TP}{TP} = 1 $$ 但曲线最终不会到（1, 0）点。很多P-R曲线的终点看着快到（1,0）点了，可能是因为负例远远多于正例，此时FP$\\gg$TP： $$Precision = \\frac{TP}{TP+FP} \\rightarrow 0 $$ ROC曲线 ROC曲线称为受试者工作特征曲线 （Receiver Operating Characteristic Curve），又称为感受性曲线（Sensitivity Curve），此名称的由来有二战相关的典故；而AUC（Area Under Curve）是ROC曲线下的面积。在ROC曲线中曲线越凸向左上角越好。 （配图从Lecture0x01中生成） 0.5 &lt; AUC &lt; 1，优于随机猜测。这个分类器（模型）妥善设定阈值的话，能有预测价值。 AUC = 0.5，跟随机猜测一样（例：丢铜板），模型没有预测价值。 AUC &lt; 0.5，比随机猜测还差。 最优零界点：保证TPR高的同时FPR要尽量的小，也即找到离（0,1）最近的点。 曲线选择1、 ROC曲线兼顾正例与负例，所以适用于评估分类器的整体性能；相比而言PR曲线完全聚焦于正例的评判。 2、如果有多份数据且存在不同的类别分布，这时候如果只想单纯地比较分类器的性能且剔除类别分布改变的影响，则ROC曲线比较适合；如果想测试不同类别分布下对分类器的性能的影响，则PR曲线比较适合。 3、相对来讲ROC曲线会稳定很多，在正负样本量都足够的情况下，ROC曲线足够反映模型的判断能力；但类别不平衡问题中，ROC曲线通常会给出一个乐观的效果估计，所以此时候还是PR曲线更好。 附录Ref. 1 木东居士/机器学习的敲门砖：kNN算法（中）Ref. 2 饼干 / 评价分类结果（上）Ref. 3 饼干 / 模型之母：线性回归的评价指标Ref. 4 云社区 / 回归模型评估指标（机器学习基础）Ref. 5 深度研究：回归模型评价指标R2_scoreRef. 6 kuaizi_sophia / 分类和回归模型常用的性能评价指标Ref. 7 大数据文摘 / 机器学习大牛最常用的5个回归损失函数，你知道几个？Ref. 8 全面了解ROC曲线Ref. 9 P-R曲线深入理解","categories":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"icyhh.xyz/categories/Machine-Learning/"}],"tags":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"icyhh.xyz/tags/Machine-Learning/"}]},{"title":"Lecture0x01_knn","slug":"Lecture0x01-knn","date":"2020-02-24T04:10:42.000Z","updated":"2020-09-23T08:41:04.955Z","comments":true,"path":"2020/02/24/Lecture0x01-knn/","link":"","permalink":"icyhh.xyz/2020/02/24/Lecture0x01-knn/","excerpt":"","text":"​ 前言：回想起来，距离第一次接触Machine Learning、做project、在组会上做分享，已经过了整整三年，前沿技术一直在发展，甚至YOLO之父 Joseph Redmon都宣布要退出CV界，而我的知识储备却并未增加。为了避免关键时刻生疏，今年初跟随饼干的学习小组，重温机器学习的轶事。 1. KNN若只如初见​ 鉴于已经不是第一次与knn约会了，不多寒暄直入主题，给它发一个身份卡（持续更新）。 名称 中文 分类模型 回归模型 关注参数 适用场景 必备技巧 K-Nearest Neighbor k最近邻算法 Yes Yes k, 相似度距离算法 特征维数低，特征以连续值为主，样本量小 归一化 2. 建模过程2-1. 数据集​ 恰逢疫情时段，我便尝试拿一份公开的疫情人数数据来分析一下，做一个简单的、可能并不太合适的建模。 ​ 数据和特征决定了机器学习的上限，而模型和算法只是逼近这个上限。对原始数据的预处理对模型结果的优劣会有关键影响，这里我先对数据集做一个简单分析： provinceName (string): 中文省名provinceEnglishName (string): 英文省名province_zipCode (string): 省邮政编码cityName (string): 中文城市名cityEnglishName (string): 英文城市名city_zipCode (string): 城市邮政编码province_confirmedCount (int): 省确诊人数province_suspectedCount (int): 省疑似人数province_curedCount (int): 省治愈人数province_deadCount (int): 省死亡人数city_confirmedCount (int): 市确诊人数city_suspectedCount (int): 市疑似人数city_curedCount (int): 市治愈人数city_deadCount (int): 市死亡人数updateTime: 更新时间 ​ 假设要预测的对象是“市死亡人数”，其他特征都为他打辅助。人数是连续值，这里需要用一个回归模型。​ 显而易见，省名、英文省、邮政编码互相之间交叉熵为零，相似度100%，三个特征一起用对结果预测没有增强作用，反而冗余，所以三选一即可。先以最简单地方式处理，留下邮政编码，去除名字特征。 ​ 特征X： province_zipCode (string): 省邮政编码city_zipCode (string): 城市邮政编码province_confirmedCount (int): 省确诊人数province_suspectedCount (int): 省疑似人数province_curedCount (int): 省治愈人数province_deadCount (int): 省死亡人数city_confirmedCount (int): 市确诊人数city_suspectedCount (int): 市疑似人数city_curedCount (int): 市治愈人数 ​ 预测对象Y： city_deadCount (int): 市死亡人数 ​ 邮政编码在这份表里有缺失，数据预处理需要先对它做个简单的数据填充（其实为避免特征噪声，数据存在的情况下对城市名进行重编码会更准确；迫不得已才做这样的数据填充）。 ​ todo. 可以改进的点有：改变非连续值特征的编码 / 对不同量纲的特征数据做归一化。 2-2. 训练模型​ 将数据集切分为训练集测试集，以便验证是否有过拟合的问题。这里可以思考的点有：k的选择[4-2](## 4-2. \\k的选择) / knn算法的距离选择4-1 / 是否对投票加权 / 数据如果自带顺序，需要shuffle一下。 2-3. 测试模型​ 验证模型的预测结果，这里可以思考的点：有评判指标的选择Lecture02 ，如何正确的验证模型的效果。 3. 手写knn3-1. 核心原理​ 一言以蔽之预测/分类思路是：找到已知数据集里与自己最相似（e.g. 欧氏距离）的k个样本，借鉴他们的结果（e.g. 回归模型则取平均/加权取平均，分类模型则投票/加权投票）。 3-2. 效果对比​ 首先展示盲选k=5时，在全数据集（训练样本数：51924，测试样本数：5770）中，sklearn封装好的knn表现： ​ 然后缩小一下数据的范围，取1%的样本数据集（训练样本数：577，测试样本数：519）中，对比一下调包与自己手动实现的效果： ​ 自己手写的knn比sklearn封装的相比，暴力遍历的方式运算耗时过分长[ 时间复杂度O(n) ]，所以不得不放弃在全数据集中做实验——计算最近邻居可以采用KDTree、BallTree等算法，有效加速；在1%的样本数据集中明显过拟合了，$R^2$ 的值在训练集上接近1，在测试集上却为0。 4. 更进一步4-1.相似度评判的方式 名称 中文 定义/用法 参考公式 适用场景 Euclidean metric 欧几里得度量 / 欧氏距离 m维空间中两个点之间的真实距离 $$ \\sqrt{(x_i-x_j)^2+(y_i-y_j)^2} $$ 适用于空间问题 Manhattan Distance 曼哈顿距离 / 出租车几何 两个点的绝对轴距总和 $$ x_i-x_j Chebyshev distance 切比雪夫距离 两个点坐标数值差(绝对值)的最大值 $$ max( x_i-x_j Mahalanobis Distance 马氏距离 测量数据的协方差距离 $$ \\sqrt{(X_i-X_j)^TS^{-1}(X_i-X_j)} $$ 量纲无关，可以排除变量之间的相关性的干扰 Bhattacharyya Distance 巴氏距离 用于测量两个离散或连续概率分布的相似性 离散概率分布：$$-ln(\\sum_{x \\in X}\\sqrt{p(x)q(x）)})$$连续概率分布：$$\\int{\\sqrt{p(x)q(x)}dx} $$ 常在分类中测量类之间的可分离性 Hamming distance 汉明距离 两个等长字符串，将其中一个变为另外一个所需要作的最小替换次数 （e.g.字符串“1111”与“1001”之间的汉明距离为2。） 信息编码（使编码间的最小汉明距离尽可能大，增强容错性） Cosine 夹角余弦 衡量两个向量方向的差异 $$ cos\\theta = \\frac{x_ix_j+y_iy_j}{\\sqrt{x_i^2+y_i^2}\\sqrt{x_j^2+y_j^2}} $$ 数据挖掘中衡量样本向量之间的差异 Pearson Correlation Coefficient 皮尔森相关系数 反映两个变量线性相关程度的统计量 $$ \\rho_{XY} = \\frac{E[X-E(X)(Y-E(Y))]} {\\sqrt{(D(X)}\\sqrt{D(Y)}} $$ 期望：$$E(X) = \\frac{\\sum{{i=1}^n X_i}}{n} $$ 方差：$$D(X) = E{[X-E(X)]^2} $$ 相关距离： $$1 - \\rho{XY}$$ 线性相关系数 Jaccard similarity coefficient 杰卡德相似系数 用两个集合中不同元素占所有元素的比例来衡量两个集合的区分度 $$J(A,B)= \\frac{ A\\cap B ​ 此外提一下，闵可夫斯基距离（Minkowski Distance）是一组距离的定义，公式中有一个变参p： $$ \\sqrt[p]{\\sum_{k=1}^n (x_{ik}-x_{jk})^p} $$ 当p=1时，是曼哈顿距离； 当p=2时，是欧氏距离； 当p→∞时，就是切比雪夫距离。 ​ 其他需要注意的 / 经验记录： 值域越大的变量常常会在距离计算中占据主导作用，因此应先对变量进行归一化。 当变量数越多，欧式距离的区分能力就越差。 4-2.k的选择 如果k太小，分类结果易受噪声点影响，误差会增大； 如果k太大，近邻中又可能包含太多的其它类别的点（对距离加权，可以降低k值设定的影响）； 在实际应用中，K值一般取一个比较小的数值，例如采用交叉验证法（cross-validation）来选择最优的K值。 经验规则：k一般低于训练样本数的平方根。 附录Ref. 0 https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsRegressor.htmlRef. 1 木东居士 / 机器学习的敲门砖：初探kNN算法Ref. 2 数月亮 / 机器学习-KNN算法Ref. 3 JokerChange / KNN浅析（距离、k值等）Ref. 4 常见距离公式Ref. 5 lijfrank / 几种常见的距离计算公式","categories":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"icyhh.xyz/categories/Machine-Learning/"}],"tags":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"icyhh.xyz/tags/Machine-Learning/"},{"name":"Algorithm","slug":"Algorithm","permalink":"icyhh.xyz/tags/Algorithm/"}]},{"title":"写在2020伊始","slug":"写在2020伊始","date":"2020-02-15T13:16:18.000Z","updated":"2020-09-23T08:41:04.957Z","comments":true,"path":"2020/02/15/写在2020伊始/","link":"","permalink":"icyhh.xyz/2020/02/15/%E5%86%99%E5%9C%A82020%E4%BC%8A%E5%A7%8B/","excerpt":"","text":"​ 时间飞逝，距离2017年元旦开始用hexo捣腾博客，已经过了三年。 ​ 鼠年开年不利，受疫情所困在家中老老实实地宅了一个月，工作之余意外有了很多安静反思的时间，便打算一鼓作气地将这个想了很久的个人知识体系建立起来，算是一个自我总结也是温故知新的过程。毕竟学到的越多，越觉察到自己一无所知。知识好比容量，球体的容积越大，表面积就越大，接触到的非球体范围内的未知就更甚。 ​ 种一棵树最好的时间是十年前，其次是现在。希望工作多年之后回望，会觉得这段时光没有白过，沉淀会结出一树繁花。","categories":[{"name":"essay","slug":"essay","permalink":"icyhh.xyz/categories/essay/"}],"tags":[]}]}
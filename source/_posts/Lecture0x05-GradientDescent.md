---
title: Lecture0x05_GradientDescent
toc: true
date: 2020-03-28 17:41:07
tags: [Machine Learning]
categories: Machine Learning
---

# 从机器学习方法论说起
### WHAT
「机器学习」就是找到一种函数f(x)并进行优化， 完成预测、分类、生成等工作。
### HOW
第一步：定义一个函数集合（define a function set）
第二步：判断函数的好坏（goodness of a function）
第三步：选择最好的函数（pick the best one）

### HOW 3
pick the best one? 
「梯度下降」是目前机器学习、深度学习解决最优化问题的算法中，最核心、应用最广的方法。

# 梯度是什么
多元函数的**导数(derivative)**就是**梯度(gradient)**，分别对每个变量进行微分。梯度是向量(有方向)，其维度与参数的维度一样。梯度指向误差值增加最快的方向。

对于**损失函数L**，$f(x_1, x_2)$，梯度为$\nabla L = (\frac{\partial L}{\partial x_1},\frac{\partial L}{\partial x_2})$



# 为什么靠梯度下降

从数学的角度出发，针对损失函数$L$，假设选取的初始点为$(a_0, b_0)$，将这个点稍微移动一点点，得到$(a_1, b_1)$。

移动一点点的增量 $\Delta L = L(a_1, b_1) - L(a_0, b_0) \approx \frac{\partial L}{\partial a}\Delta a + \frac{\partial L}{\partial b}\Delta b$  （代入了泰勒展开式）

如果令移动的距离$\Delta a$、$\Delta b$分别为$-\eta\frac{\partial L}{\partial a}$、$-\eta\frac{\partial L}{\partial b}$，其中规定$\eta > 0$，则：

 $\Delta L  \approx -\eta (\frac{\partial L}{\partial a})^2 + (\frac{\partial L}{\partial b})^2 \leqslant 0$ 

我们如果按照<u>规定的移动距离公式</u>移动参数，那么损失函数的函数值始终是下降的，这样就达到了我们要求的“损失变小”的要求了。**如果一直重复这种移动，则可以证明损失函数最终能够达到一个最小值。**

所以下一个点的迭代公式：

$(a_{k+1}, b_{b+1})=(a_{k}+\Delta a, b_{b}+\Delta b)) = (a_{k}-\eta\frac{\partial L}{\partial a}, b_{b}-\eta\frac{\partial L}{\partial b}))$



## 为什么梯度要乘以一个负号

梯度的方向就是损失函数值**在此点上升最快的方向**，是损失增大的区域；而我们要使损失最小，因此就要逆着梯度方向走，自然就是负的梯度的方向。



## 关于参数$\eta$

我们已经知道，梯度对应的是下山的方向，而参数 对应的是步伐的长度。在学术上，我们称之为**“学习率”(learning rate)**，是模型训练时的一个很重要的超参数，**能直接影响算法的正确性和效率**：

1. 学习率$\eta$不能太大。从数学角度上来说，一阶泰勒公式只是一个近似的公式，只有在学习率很小，也就是$\Delta a$, $\Delta b$很小时才成立。直观上考虑，如果学习率太大，那么有可能会“迈过”最低点，从而发生“摇摆”的现象（不收敛），无法得到最低点。
2. 学习率又不能太小。如果太小，会导致每次迭代时，参数几乎不变化，收敛学习速度变慢，使得算法的效率降低，需要很长时间才能达到最低点。



 # 梯度下降算法的缺点

从理论上，它只能保证达到局部最低点，而非全局最低点。在很多复杂函数中有很多极小值点，我们使用梯度下降法只能得到局部最优解，而不能得到全局最优解。

对应的**解决方案**有：首先**随机产生多个初始参数集**，即多组$a_{0}$, $b_{0}$；然后**分别对每个初始参数集使用梯度下降法**，直到函数值收敛于某个值；最后从这些值中找出最小值，这个找到的最小值被当作函数的最小值。当然这种方式不一定能找到全局最优解，但是起码能找到较好的。



# 如何实施梯度下降

//todo Ref. 2&3



# 优化梯度下降算法

| 名称                               | 中文                                                     | 特点                                                         | 优缺点                                                       |
| ---------------------------------- | -------------------------------------------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| BGD (Batch Gradient Descent)       | 批量梯度下降法                                           | 通过对整个数据集的所有样本的计算来求解梯度的方向。           | 在数据量很大时需要计算很久                                   |
| SGD (Stochastic Gradient Descent） | 随机梯度下降法                                           | 每次迭代使用一个样本来对参数进行更新。虽然不是每次迭代得到的损失函数都向着全局最优方向，但是大的整体的方向是向全局最优解的。学习率$\eta$需要逐渐递减的。 | 相比于批量梯度，这样的方法更快。但随机梯度下降有着不可预知性。 |
| SGDM (SGD with Momentum)           |                                                          | 当前权值的改变会收到上一次权值的改变的影响，就像小球滚动时候一样，由于惯性，当前状态会受到上一个状态影响，这样可以加快速度。 |                                                              |
| AdaGrad (Adative Gradient Descent) | 自适应梯度                                               | 核心思想是对于常见的数据给予比较小的学习率去调整参数，对于不常见的数据给予比较大的学习率调整参数。它可以自动调节学习率，但迭代次数多的时候，学习率也会下降。 | 加大小梯度方向的进步，自动调节学习率；但是时间太长的话，会造成步长太小，困在局部极值点。 |
| RMSProp                            | 均方差传播                                               | 基于权重梯度最近量级的均值为每一个参数适应性地保留学习率。这意味着算法在非稳态和在线问题上有很有优秀的性能。 |                                                              |
| Adam                               | 它的名称来源于适应性矩估计（adaptive moment estimation） | AdaGrad通过计算梯度的一阶矩估计和二阶矩估计而为不同的参数设计独立的自适应性学习率。 | 同时获得了 AdaGrad 和 RMSProp 算法的优点。                   |

//to be continue.



# 附录

Ref. 1 Japson / [还不了解梯度下降法？看完这篇就懂了！](https://mp.weixin.qq.com/s/44p8anqiiQV6XYGqH5u-Ug)

Ref.2  Japson /  [手动实现梯度下降（可视化）](https://mp.weixin.qq.com/s/nI9IBa4ccfg0xqyn0tbRPA)

Ref.3  Japson / [线性回归中的梯度下降](https://mp.weixin.qq.com/s/8gStYSSBvkXeuaX6Pp9qiQ)

Ref.4  Japson /[速度更快的随机梯度下降法](https://mp.weixin.qq.com/s/OUslRwKGpS29gncsiyAPyg)

Ref.5  Japson /[梯度下降番外：非常有用的调试方式及总结](https://mp.weixin.qq.com/s/CL5GZKGHPaUf9MW2d08C2A)

